<!doctype html>
<html lang="en">
	<head>
	    <meta charset="UTF-8">
		<title>Image to text using Thunkable app</title>
	</head>
	<body>
		<div id="debugPanel">
			<div>Debug mode on. V1.15</div>
			<button type="button" id="fes" onclick="init('1')">Start</button>
			<button type="button" id="reload" onclick="location.reload()">Reload</button>
			<div id="informationLabel"></div>
		</div>
		<video id="myVideo"></div>
		<!-- The canvas width and height must coincide with the camera dimensions -->
		<canvas id="theCanvas" width="1280" height="720"></canvas>
	</body>

	<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.3.1/dist/tf.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/@teachablemachine/image@0.8/dist/teachablemachine-image.min.js"></script>
	<script src="https://thunkable.github.io/webviewer-extension/thunkableWebviewerExtension.js" type="text/javascript"></script>
	<script type="text/javascript">

		// -----------------------
		// Const definitions
		// -----------------------
		// Set it to true to see a label with the initialization process and the text interpreted
		const _DEBUG_ = false;
		document.getElementById("debugPanel").hidden = !_DEBUG_;

		// URL base where the teachable machine model and metadata is
		const _URL_ = "./Jo/";
		// Set _USE_AUDIO_ to true to enable the audio in the user media
		const _USE_AUDIO_ = false;

		// To control the hysteresis
		// Time which is considered a stable image (in milliseconds)
		const timeConsiderStable_ = 500

		// -----------------------
		// Variables
		// -----------------------
		// Image canvas and video
		var theCanvas = document.getElementById("theCanvas");
		var ctx       = theCanvas.getContext('2d');
		ctx.canvas.hidden = true;
		var video = document.getElementById("myVideo");

		// Model used and number of classes defined in the model
		var theModel_, numClassesDefined_;

		// Last text found
		var lastTextFound_ = "";
		// Time that the predicted text changed
		var lastTimeTextChanged_ = 0;
		// If the actual text was sent
		var textSent_ = false;

		function setMessage(msg)
		{
			if (_DEBUG_)
			{
				document.getElementById("informationLabel").innerHTML = msg;
			}
		}

		function appendMessage(msg)
		{
			if (_DEBUG_)
			{
				document.getElementById("informationLabel").innerHTML += msg;
			}
		}

		// Initializates the model and calls to initializate the camera
	    async function init(indexCam) 
		{
			setMessage("Initializing model");
	        const modelURL = _URL_ + "model.json";
		    const metadataURL = _URL_ + "metadata.json";
		    theModel_ = await tmImage.load(modelURL, metadataURL);
	        numClassesDefined_ = theModel_.getTotalClasses();
			setMessage("Initializing system");
			try
			{
				// Once the model is initiated, we init the camera using the index device
				await initCam(indexCam);
			}
			catch (error)
			{
				setMessage("Error initializing Cam. " + error);
			}
		}

	    // Load the image model and setup the webcam
		async function initCam(indexCam) 
		{
			setMessage("Initializing CAM");

			navigator.mediaDevices.enumerateDevices()
				.then(function(devices) {
					var cameras = [];
					devices.forEach(function(device) {
						'videoinput' === device.kind && cameras.push(device.deviceId);
					});
					var msg = "Cams found:<br>";
					var br="";
					for (var iCnt = 0; iCnt<cameras.length; iCnt++)
					{
						msg += br;
						msg += "DeviceId(";
						msg += iCnt;
						msg += "):";
						msg += cameras[iCnt];
						br = "<br>";
					}
					setMessage(msg);
  					continueCam(cameras[parseInt(indexCam)]);
				})
				.catch(function(err) 
				{
					setMessage(err.name + ": " + err.message);
				});
		}

		async function continueCam(idCamera)
		{
			setMessage("Using camera: " + idCamera);
			const constraints2 = { audio: _USE_AUDIO_, 
								   video: {  
								     width: 1280,
								     height: 720,
								     deviceId: {exact : idCamera}
								   } 
							     };

			navigator.mediaDevices.getUserMedia(constraints2)
				.then(function(mediaStream) {
					setMessage("CAM Initialized");
					video.srcObject = mediaStream;
					video.onloadedmetadata = function(e) {
						video.play();
						window.requestAnimationFrame(loop);
					};
				})
				.catch(function(err) { setMessage(err.name + ": " + err.message); }); // always check for errors at the end.
		}

		// The loop will capturate the video image, pass it to the canvas and
		// call to the predict model function
		// Once we have the result, we search for the more probabile
		// Do an hysteresis algorithm and if the text changed, send the message to the thunkable extension
	    async function loop() 
		{
			ctx.drawImage(video, 0, 0);
			const prediction = await theModel_.predict(theCanvas);
			max = 0;
			index = 0;
			var res = ""
			for (let i = 0; i < numClassesDefined_; i++) 
			{
				res += prediction[i].className + "("+prediction[i].probability.toFixed(2)+")<br>"
				if (max < prediction[i].probability.toFixed(2))
				{
					max = prediction[i].probability.toFixed(2);
					index = i;
				}
			}

			var currTextFound = prediction[index].className;
			if (lastTextFound_ == currTextFound)
			{
				var currTime = Date.now();
				if (currTime - lastTimeTextChanged_ >= timeConsiderStable_ && !textSent_)
				{
					textSent_ = true;
					ThunkableWebviewerExtension.postMessage(currTextFound);
				}
			}
			else
			{
				lastTimeTextChanged_ = Date.now();
				lastTextFound_ = currTextFound;
				textSent_ =  false;
			}

			// Execute the loop forever
			window.requestAnimationFrame(loop);
		}

		// Here we have all the messages that receives from the thunkable program
		// The InitCam message is needed to initiate the system. It will receive the number
		// of the camera to initiate just after the InitCam literal.
		// For instance InitCam1
		ThunkableWebviewerExtension.receiveMessage(function(missatge) 
		{
			setMessage(missatge);
			if (missatge.substring(0,7) == "InitCam")
			{
				init(missatge.substring(7,8));
			}
		});

	</script>
</html>